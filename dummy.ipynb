{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the arXiv ID for SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARXIV:2309.13625',\n",
       " 'ARXIV:2310.13023',\n",
       " 'ARXIV:2310.04560',\n",
       " 'ARXIV:2310.04944',\n",
       " 'ARXIV:2310.00299']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "datas = []\n",
    "unmatch = []\n",
    "# Regular expression pattern to find and capture the numerical parts\n",
    "pattern = r\"https://arxiv.org/pdf/(\\d{4}\\.\\d{5})\\.pdf\"\n",
    "\n",
    "\n",
    "with open('data/reason.md', 'r') as f:\n",
    "    for line in f.readlines():  \n",
    "        if 'https://arxiv' in line:\n",
    "            # Find all matches in the string\n",
    "            matches = re.findall(pattern, line)\n",
    "            datas.append(f'ARXIV:{matches[0]}')\n",
    "        elif 'pdf' in line:\n",
    "            unmatch.append(line)\n",
    "\n",
    "datas[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get TL;DR from SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "sch = SemanticScholar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_data(paper):\n",
    "    domains = ['title', 'authors', 'abstract', 'tldr', 'venue', 'referenceCount', 'citationCount', 'influentialCitationCount']\n",
    "\n",
    "    meta_data = {}\n",
    "    for domain in domains:\n",
    "        if domain in paper.keys():\n",
    "            if domain == 'authors':\n",
    "                authors = []\n",
    "                for author_domain in paper.__getattribute__(domain):\n",
    "                    authors.append(author_domain['name'])\n",
    "                meta_data['authors'] = authors\n",
    "            else:\n",
    "                domain_res = paper.__getattribute__(domain)\n",
    "                if domain == 'tldr':\n",
    "                    domain_res = str(domain_res)\n",
    "                meta_data[domain] = domain_res\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/111 [00:22<22:41, 12.49s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import time\n",
    "\n",
    "meta_datas = []\n",
    "with open('result.json', 'a+') as fp:\n",
    "    for data in tqdm(datas):\n",
    "        try:\n",
    "            paper = sch.get_paper(data)\n",
    "            fp.writelines(get_meta_data(paper))\n",
    "        except:\n",
    "            fp.writelines({'Cannot find': data})\n",
    "        \n",
    "        meta_datas.append(get_meta_data(paper))\n",
    "        time.sleep(2) # hold 2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models 2024-02-29 18:59:58+00:00\n",
      "Single Electron Quantum Dot in Two-Dimensional Transition Metal Dichalcogenides 2024-02-29 18:59:57+00:00\n",
      "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers 2024-02-29 18:59:50+00:00\n",
      "Learning a Generalized Physical Face Model From Data 2024-02-29 18:59:31+00:00\n",
      "Impact of weak lensing on bright standard siren analyses 2024-02-29 18:59:30+00:00\n",
      "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations? 2024-02-29 18:59:25+00:00\n",
      "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World 2024-02-29 18:59:17+00:00\n",
      "Retrieval-Augmented Generation for AI-Generated Content: A Survey 2024-02-29 18:59:01+00:00\n",
      "Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress 2024-02-29 18:58:26+00:00\n",
      "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling 2024-02-29 18:58:15+00:00\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "# -----\n",
    "# sort_by\n",
    "# Relevance \n",
    "# LastUpdatedDate \n",
    "# SubmittedDate\n",
    "# -----\n",
    "# order\n",
    "# Ascending\n",
    "# Descending\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Large Language Models\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "  sort_order = arxiv.SortOrder.Descending,\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "for r in client.results(search):\n",
    "  print(r.title, r.published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models\n",
      "Single Electron Quantum Dot in Two-Dimensional Transition Metal Dichalcogenides\n",
      "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers\n",
      "Learning a Generalized Physical Face Model From Data\n",
      "Impact of weak lensing on bright standard siren analyses\n",
      "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?\n",
      "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World\n",
      "Retrieval-Augmented Generation for AI-Generated Content: A Survey\n",
      "Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress\n",
      "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling\n",
      "['DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models', 'Single Electron Quantum Dot in Two-Dimensional Transition Metal Dichalcogenides', 'Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers', 'Learning a Generalized Physical Face Model From Data', 'Impact of weak lensing on bright standard siren analyses', 'The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?', 'The All-Seeing Project V2: Towards General Relation Comprehension of the Open World', 'Retrieval-Augmented Generation for AI-Generated Content: A Survey', 'Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress', 'Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling']\n",
      "http://arxiv.org/abs/cond-mat/0603029v1\n",
      "From stripe to checkerboard order on the square lattice in the presence of quenched disorder\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = \"Large Language Models\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "  print(r.title)\n",
    "# ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
    "all_results = list(results)\n",
    "print([r.title for r in all_results])\n",
    "\n",
    "# For advanced query syntax documentation, see the arXiv API User Manual:\n",
    "# https://arxiv.org/help/api/user-manual#query_details\n",
    "search = arxiv.Search(query = \"au:del_maestro AND ti:checkerboard\")\n",
    "first_result = next(client.results(search))\n",
    "print(first_result)\n",
    "\n",
    "# Search for the paper with ID \"1605.08386v1\"\n",
    "search_by_id = arxiv.Search(id_list=[\"1605.08386v1\"])\n",
    "# Reuse client to fetch the paper, then print its title.\n",
    "first_result = next(client.results(search))\n",
    "print(first_result.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_and_categorize_papers(venue_id):\n",
    "    url = f\"https://api2.openreview.net/notes?content.venueid={venue_id}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Initialize dictionaries to hold categorized papers\n",
    "    papers_by_type = {'Oral': [], 'Spotlight': [], 'Poster': []}\n",
    "    \n",
    "    # Iterate over all papers and categorize them\n",
    "    if 'notes' in data:\n",
    "        for note in data['notes']:\n",
    "            venue_info = note['content'].get('venue', {})\n",
    "            if 'value' in venue_info:\n",
    "                venue_value = venue_info['value']\n",
    "                if 'oral' in venue_value.lower():\n",
    "                    papers_by_type['Oral'].append(note)\n",
    "                elif 'spotlight' in venue_value.lower():\n",
    "                    papers_by_type['Spotlight'].append(note)\n",
    "                elif 'poster' in venue_value.lower():\n",
    "                    papers_by_type['Poster'].append(note)\n",
    "    \n",
    "    return papers_by_type\n",
    "\n",
    "# Usage of the function\n",
    "venue_id = \"ICLR.cc/2024/Conference\"\n",
    "\n",
    "# venue_id = \"NeurIPS.cc/2023/Conference\"\n",
    "papers_by_type = fetch_and_categorize_papers(venue_id)\n",
    "\n",
    "# Print the results to verify\n",
    "# for paper_type, notes in papers_by_type.items():\n",
    "#     print(f\"\\n{paper_type} Papers:\")\n",
    "#     if notes:\n",
    "#         for note in notes[:5]:  # Limiting to first 5 papers for brevity\n",
    "#             title = note.get('content', {}).get('title', 'No title available')\n",
    "#             authors = \", \".join(note.get('content', {}).get('authors', []))\n",
    "#             abstract = note.get('content', {}).get('abstract', 'No abstract available')\n",
    "#             tldr = note.get('content', {}).get('TLDR', 'No TL;DR available')\n",
    "#             keywords = note.get('content', {}).get('keywords', 'No keywords available')\n",
    "#             link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "#             print(f\"Title: {title}\")\n",
    "#             print(f\"Authors: {authors}\")\n",
    "#             print(f\"Abstract: {abstract}\")\n",
    "#             print(f\"TL;DR: {tldr}\")\n",
    "#             print(f\"Keywords: {keywords}\")\n",
    "#             print(f\"Link: {link}\")\n",
    "#             print(\"---\")\n",
    "#     else:\n",
    "#         print(\"No papers found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oral Papers:\n",
      "Title: Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement\n",
      "Authors: Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: language model; natural language processing; inductive reasoning\n",
      "---\n",
      "Title: MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts\n",
      "Authors: Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao\n",
      "TL;DR: We introduce MathVista, a novel benchmark for evaluating mathematical reasoning capabilities within visual contexts, and conduct extensive experiments on 11 foundation models.\n",
      "Keywords: large language models; large multimodal models; mathematical reasoning; vision-language reasoning; foundation models and their evaluations\n",
      "---\n",
      "Title: SWE-bench: Can Language Models Resolve Real-world Github Issues?\n",
      "Authors: Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan\n",
      "TL;DR: A novel benchmark for evaluating language models that introduces software engineering as a task.\n",
      "Keywords: Language models; Natural language processing; Software engineering\n",
      "---\n",
      "Title: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "Authors: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi\n",
      "TL;DR: We introduce Self-RAG, a new training and inference framework to enable an LM learn to retrieve, generate and critique.\n",
      "Keywords: Retrieval-augmented Generation; Language Models; Retrieval-augmented LMs; Factuality\n",
      "---\n",
      "\n",
      "Spotlight Papers:\n",
      "Title: The Consensus Game: Language Model Generation via Equilibrium Search\n",
      "Authors: Athul Paul Jacob, Yikang Shen, Gabriele Farina, Jacob Andreas\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: language models; decoding; planning; game theory\n",
      "---\n",
      "Title: MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning\n",
      "Authors: Zayne Rea Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett\n",
      "TL;DR: We introduce MuSR, a new dataset for testing LLMs' abilities to do complex, structured reasoning based on generated narratives.\n",
      "Keywords: Large Language Models; Chain-of-Thought; Textual Reasoning\n",
      "---\n",
      "Title: In-Context Pretraining: Language Modeling Beyond Document Boundaries\n",
      "Authors: Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Wen-tau Yih, Mike Lewis\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models\n",
      "---\n",
      "Title: Tool-Augmented Reward Modeling\n",
      "Authors: Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua Wu\n",
      "TL;DR: This paper introduces tool-augmented reward models for reinforcement learning from human feedback (RLHF), improving precision and interpretability and contributing a comprehensive dataset from seven diverse tool APIs to advance the field.\n",
      "Keywords: Reward Model; Large Language Model; Tool Learning; Augmented Language Model\n",
      "---\n",
      "Title: Spatially-Aware Transformers for Embodied Agents\n",
      "Authors: Junmo Cho, Jaesik Yoon, Sungjin Ahn\n",
      "TL;DR: We propose a transformer-based episodic memory model, the Spatially-Aware Episodic Transformer, that incorporates both temporal and spatial dimensions to improve memory utilization and downstream task accuracy.\n",
      "Keywords: Episodic Memory; Spatial Inference; Prediction; Generation; Reinforcement Learning\n",
      "---\n",
      "Title: Grounding Language Plans in Demonstrations Through Counterfactual Perturbations\n",
      "Authors: Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Grounding LLM; Learning Mode Abstractions for Manipulation; Learning from Demonstration; Robotics; Task and Motion Planning\n",
      "---\n",
      "Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents\n",
      "Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap\n",
      "TL;DR: SOTOPIA is a novel, challenging, and interactive benchmark that could serve as the perfect test-bed and potential incubator for social intelligence.\n",
      "Keywords: Social; Interaction; Agent; Social intelligence; Large Language Models; Evaluation; Theory of Mind\n",
      "---\n",
      "Title: EQA-MX: Embodied Question Answering using Multimodal Expression\n",
      "Authors: Md Mofijul Islam, Alexi Gladstone, Riashat Islam, Tariq Iqbal\n",
      "TL;DR: We present EQA-MX, a dataset and benchmark tasks for embodied multimodal QA, and VQ-Fusion model, enhancing visual-language alignment that outperforming existing models by 13%.\n",
      "Keywords: multimodal representation learning; visual-language models; embodied question answering\n",
      "---\n",
      "Title: On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods\n",
      "Authors: Montgomery Bohde, Meng Liu, Alexandra Saxton, Shuiwang Ji\n",
      "TL;DR: This work reveals the importance of aligning model design with the Markov nature in neural algorithmic reasoning tasks.\n",
      "Keywords: Neural Algorithmic Reasoning\n",
      "---\n",
      "Title: DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines\n",
      "Authors: Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts\n",
      "TL;DR: We propose a programming model that unifies LM prompting, finetuning, and augmentation techniques. We evaluate simple strategies to bootstrap and optimize complex and multi-stage reasoning chains, establishing strong results with small and large LMs.\n",
      "Keywords: programming models; prompting techniques; in-context learning; few-shot learning; chain of thought; multi-hop reasoning; language agents\n",
      "---\n",
      "Title: Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics\n",
      "Authors: Christian Gumbsch, Noor Sajid, Georg Martius, Martin V. Butz\n",
      "TL;DR: We propose an algorithm to learn a hierarchy of world models from sparse latent state changes for explainable, long-horizon planning.\n",
      "Keywords: world models; temporal abstraction; hierarchical learning; model-based reinforcement learning; hierarchical planning\n",
      "---\n",
      "\n",
      "Poster Papers:\n",
      "Title: Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models\n",
      "Authors: Sijia Chen, Baochun Li, Di Niu\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models; Prompt Engineering; Boosting Mechanism;\n",
      "---\n",
      "Title: Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation\n",
      "Authors: Niels Mündler, Jingxuan He, Slobodan Jenko, Martin Vechev\n",
      "TL;DR: We present a comprehensive analysis showing that state-of-the-art LLMs frequently produce self-contradictory hallucinations. We then design prompting methods that effectively detect and mitigate self-contradictions.\n",
      "Keywords: language model; hallucination; trustworthy artificial intelligence; reasoning\n",
      "---\n",
      "Title: RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "Authors: Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning\n",
      "TL;DR: RAPTOR improves LLM QA performance by constructing a hierarchical summarization tree for information retrieval, outperforming existing retrieval methods across various metrics and datasets.\n",
      "Keywords: Retrieval Augmented Language Models; Information Retrieval; summarization; QA\n",
      "---\n",
      "Title: Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources\n",
      "Authors: Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, Lidong Bing\n",
      "TL;DR: We present chain-of-knowledge, a novel framework that augments large language models dynamically by incorporating grounding information from heterogeneous sources.\n",
      "Keywords: large language model; knowledge grounding\n",
      "---\n",
      "Title: Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n",
      "Authors: Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot\n",
      "TL;DR: Assigning personas to LLMs can bring their deep-rooted biases to the surface, significantly diminishing their reasoning ability across domains.\n",
      "Keywords: Bias; Fairness; LLM; Reasoning; Persona; Safety\n",
      "---\n",
      "Title: Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models\n",
      "Authors: Yuanfeng Ji, Chongjian GE, Weikai Kong, Enze Xie, Zhengying Liu, Zhenguo Li, Ping Luo\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: LLMs; VLMs; Benchmark\n",
      "---\n",
      "Title: Are Bert Family Good Instruction Followers?  A Study on Their Potential And Limitations\n",
      "Authors: yisheng xiao, Juntao Li, Zechen Sun, Zechang Li, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Min Zhang\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Instruction tuning; Large language models; BERT family; Natural language generation\n",
      "---\n",
      "Title: Neural-Symbolic Recursive Machine for Systematic Generalization\n",
      "Authors: Qing Li, Yixin Zhu, Yitao Liang, Ying Nian Wu, Song-Chun Zhu, Siyuan Huang\n",
      "TL;DR: We present Neural-Symbolic Recursive Machine for systematic generalization, which achieves state-of-the-art performance on SCAN, PCFG, and HINT.\n",
      "Keywords: Neuro-symbolic AI; Systematic Generalization; Compositional Generalization\n",
      "---\n",
      "Title: Chain of Thought Empowers Transformers to Solve Inherently Serial Problems\n",
      "Authors: Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma\n",
      "TL;DR: We show both theoretically and empirically transformers with polynomial steps of CoT can simulate polysize circuits and thus are strictly more expressive than transformers without CoT.\n",
      "Keywords: Chain of thought; language modeling; circuit complexity; deep learning theory\n",
      "---\n",
      "Title: PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning\n",
      "Authors: Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona Jacqueline Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, Yejin Choi\n",
      "TL;DR: We propose a novel two-pronged approach to endow small language models with procedural knowledge and (constrained) language-based planning capabilities through knowledge distillation and an inference-time algorithm.\n",
      "Keywords: language-based planning; procedural/script knowledge; distillation; large language models; decoding-time algorithm\n",
      "---\n",
      "Title: FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\n",
      "Authors: Daniel Y Fu, Hermann Kumbong, Eric Nguyen, Christopher Re\n",
      "TL;DR: We propose FlashFFTConv, a new system that optimizes the FFT convolution algorithm to speed up long convolutions and enable long-sequence applications.\n",
      "Keywords: convolutions; GPUs; hardware-efficient algorithms; long context; fast fourier transform; I/O awareness\n",
      "---\n",
      "Title: Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers\n",
      "Authors: Awni Altabaa, Taylor Whittington Webb, Jonathan D. Cohen, John Lafferty\n",
      "TL;DR: An extension of Transformers is proposed for explicit representation of relational information.\n",
      "Keywords: relational representation learning; attention; transformers; sequence models; abstract representations\n",
      "---\n",
      "Title: Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning\n",
      "Authors: Ahmed Abdulaal, adamos hadjivasiliou, Nina Montana-Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C. Castro, Daniel C. Alexander\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Causal Reasoning; Causal Discovery; Structural Causal Models; Large Language Models\n",
      "---\n",
      "Title: BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models\n",
      "Authors: Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li\n",
      "TL;DR: We proposed the first backdoor attack against LLMs with COT prompting that does not require access to the training set or model parameters.\n",
      "Keywords: large language model; chain-of-thought; backdoor attack; reasoning task\n",
      "---\n",
      "Title: Llemma: An Open Language Model for Mathematics\n",
      "Authors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: reasoning; language models; pretraining\n",
      "---\n",
      "Title: Can Large Language Models Infer Causation from Correlation?\n",
      "Authors: Zhijing Jin, Jiarui Liu, Zhiheng LYU, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, Bernhard Schölkopf\n",
      "TL;DR: We propose Corr2Cause, the first dataset to infer causation from correlation by pure reasoning, and test 17 LLMs' performance on it.\n",
      "Keywords: Large Language Models; Natural Language Inference; Causal Reasoning; Correlation-to-Causation Inference; Benchmark Dataset; Causal Discovery\n",
      "---\n",
      "Title: REFACTOR: Learning to Extract Theorems from Proofs\n",
      "Authors: Jin Peng Zhou, Yuhuai Wu, Qiyang Li, Roger Baker Grosse\n",
      "TL;DR: We extract useful mathematical theorems using graph neural networks, evaluating on several downstream tasks to demonstrate their great utility.\n",
      "Keywords: theorem extraction; mathematical reasoning; theorem proving\n",
      "---\n",
      "Title: Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization\n",
      "Authors: Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy, Kilian Q Weinberger, Yuhuai Wu\n",
      "TL;DR: We show that automatically formalizing and verifying LLM generated quantitative reasoning solutions consistently outperforms vanilla majority voting.\n",
      "Keywords: mathematical reasoning; autoformalization; automated theorem proving; quantitative reasoning\n",
      "---\n",
      "Title: Understanding Catastrophic Forgetting in Language Models via Implicit Inference\n",
      "Authors: Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan\n",
      "TL;DR: Fine-tuning may be understood as changing how a model infers the task of the prompt, and this allows us to recover the pretrained capabilities of language models through conjugate prompting.\n",
      "Keywords: implicit inference in language models; fine-tuning; catastrophic forgetting\n",
      "---\n",
      "Title: Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks\n",
      "Authors: Ben Eisner, Yi Yang, Todor Davchev, Mel Vecerik, Jonathan Scholz, David Held\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Learning from Demonstration; Manipulation; 3D Learning; SE(3) Equivariance\n",
      "---\n",
      "Title: Lemur: Integrating Large Language Models in Automated Program Verification\n",
      "Authors: Haoze Wu, Clark Barrett, Nina Narodytska\n",
      "TL;DR: We present a general methodology for combining LLMs and formal verifiers for automated program verification.\n",
      "Keywords: Large Language Models; Formal verification\n",
      "---\n",
      "Title: CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models\n",
      "Authors: Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Reddy Evuru, Ramaneswaran S, S Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: audio; audio-language; compositional reasoning\n",
      "---\n",
      "Title: COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits\n",
      "Authors: Mintong Kang, Nezihe Merve Gürel, Linyi Li, Bo Li\n",
      "TL;DR: We propose a certifiably robust learning-reasoning pipeline for conformal prediction.\n",
      "Keywords: conformal prediction; adversarial robustness; probabilistic circuits\n",
      "---\n",
      "Title: Contrastive Difference Predictive Coding\n",
      "Authors: Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach\n",
      "TL;DR: a temporal difference version of contrastive predictive coding\n",
      "Keywords: contrastive learning; reinforcement learning; goal-reaching; goal-conditioned RL; temporal difference\n",
      "---\n",
      "Title: Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning\n",
      "Authors: Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, Kan Li\n",
      "TL;DR: We propose early-stopping self-consistency (ESC), a simple and scalable process which is capable of greatly reducing the cost of SC while maintaining performance.\n",
      "Keywords: Self-consistency; Chain-of-Thoughts; Multi-Step Reasoning; Large Language Models\n",
      "---\n",
      "Title: LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation\n",
      "Authors: Suhyeon Lee, Won Jun Kim, Jinho Chang, Jong Chul Ye\n",
      "TL;DR: We present a state-of-the-art multimodal LLM for chest X-ray understanding and generation, developed using a method that builds upon the transformer+VQGAN architecture and adapts it for instruction-finetuning of an LLM pretrained only on text.\n",
      "Keywords: large language model; multimodal; medical imaging; chest X-ray; bidirectional; instruction-tuning; vision-question answering\n",
      "---\n",
      "Title: Making Retrieval-Augmented Language Models Robust to Irrelevant Context\n",
      "Authors: Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant\n",
      "TL;DR: We present a thorough analysis of cases where retrieval augmentation hurts performance of large language models, and propose methods that improve their robustness to irrelevant context, thereby increasing their overall performance.\n",
      "Keywords: Retrieval Augmented Language Models; Large Language Models; Robustness; Question Answering\n",
      "---\n",
      "Title: AgentBench: Evaluating LLMs as Agents\n",
      "Authors: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang\n",
      "TL;DR: The first systematic benchmark for evaluating LLMs as Agents\n",
      "Keywords: Large language models; Autonomous agents; Reasoning; Evaluation; Benchmark\n",
      "---\n",
      "Title: ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search\n",
      "Authors: Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, Chao Zhang\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Model; Tool Use; Tree Search; A* Search\n",
      "---\n",
      "Title: What Algorithms can Transformers Learn? A Study in Length Generalization\n",
      "Authors: Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, Preetum Nakkiran\n",
      "TL;DR: We show that length generalization of Transformer models trained from scratch strongly correlates with the simplicity of the true RASP-L program for the task.\n",
      "Keywords: length generalization; systematic generalization; understanding; transformer; scratchpad; LLM; algorithmic reasoning\n",
      "---\n",
      "Title: Understanding Expressivity of GNN in Rule Learning\n",
      "Authors: Haiquan Qiu, Yongqi Zhang, Yong Li, quanming yao\n",
      "TL;DR: We analyze the expressivity of SOTA GNNs for KG reasoning by deducing the rule strcutures they can learn.\n",
      "Keywords: Graph Neural Networks; KG reasoning; Link prediction; Rule learning; Expressivity\n",
      "---\n",
      "Title: COLLIE: Systematic Construction of Constrained Text Generation Tasks\n",
      "Authors: Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik R Narasimhan\n",
      "TL;DR: We propose a new framework for the systematic construction of challenging instances for constrained text generation.\n",
      "Keywords: constrained text generation; large language models; compositional benchmark\n",
      "---\n",
      "Title: GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules\n",
      "Authors: Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large language models; Neuro-symbolic Visual Reasoning\n",
      "---\n",
      "Title: Generating Pragmatic Examples to Train Neural Program Synthesizers\n",
      "Authors: Saujas Vaduguru, Daniel Fried, Yewen Pu\n",
      "TL;DR: Pragmatic program synthesis in a realistic program space without human supervision in training\n",
      "Keywords: program synthesis; pragmatics; self-play\n",
      "---\n",
      "Title: CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding\n",
      "Authors: Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, Yikang Shen, Chuang Gan\n",
      "TL;DR: Our proposed CoVLM improves compositional reasoning ability of VLM through dynamically communicating between vision and language, achieving the SOTA on both compositional reasoning task and traditional VL tasks.\n",
      "Keywords: vision-language model; compositionality\n",
      "---\n",
      "Title: Building Cooperative Embodied Agents Modularly with Large Language Models\n",
      "Authors: Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan\n",
      "TL;DR: We present CoELA, a modular framework integrating LLMs to address the challenging multi-agent embodied cooperation problem with decentralized control, costly communication, and long-horizon multi-objective tasks.\n",
      "Keywords: Large Language Models; Embodied Intelligence; Multi-Agent Cooperation; Human-AI Interaction; Communication\n",
      "---\n",
      "Title: Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models\n",
      "Authors: Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: visual question answering; zero-shot; large vision language models; visual reasoning; underspecification; grounding language to vision\n",
      "---\n",
      "Title: Let Models Speak Ciphers: Multiagent Debate through Embeddings\n",
      "Authors: Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang\n",
      "TL;DR: We present a novel communication approach for Large Language Models (LLMs) by removing the token sampling step from LLMs and enabling them to convey their beliefs across the vocabulary through the expectation of raw transformer output embeddings.\n",
      "Keywords: multiagent debate; large language models; inter-model communication; embedding representation\n",
      "---\n",
      "Title: Channel Vision Transformers: An Image Is Worth 1 x 16 x 16 Words\n",
      "Authors: Yujia Bao, Srinivasan Sivanandan, Theofanis Karaletsos\n",
      "TL;DR: ChannelViT facilitates robust representation learning across different input channels.\n",
      "Keywords: vision transformer; representation learning; hyper spectral imaging\n",
      "---\n",
      "Title: LLM Augmented LLMs: Expanding Capabilities through Composition\n",
      "Authors: Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models; Model Composition; Knowledge Augmentation\n",
      "---\n",
      "Title: When can transformers reason with abstract symbols?\n",
      "Authors: Enric Boix-Adserà, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua M. Susskind\n",
      "TL;DR: Transformers learn to reason relationally given enough data, and we improve data efficiency with theory-inspired architecture modifications.\n",
      "Keywords: transformers; language models; reasoning; theoretical analysis; variable binding\n",
      "---\n",
      "Title: Towards Generative Abstract Reasoning: Completing Raven’s Progressive Matrix via Rule Abstraction and Selection\n",
      "Authors: Fan Shi, Bin Li, Xiangyang Xue\n",
      "TL;DR: This paper proposes a novel deep latent variable model to solve generative RPM problems through rule abstraction and selection.\n",
      "Keywords: Deep Latent Variable Models; Generative Models; Raven’s Progressive Matrix; Abstract Visual Reasoning\n",
      "---\n",
      "Title: Think before you speak: Training Language Models With Pause Tokens\n",
      "Authors: Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan\n",
      "TL;DR: We explore delaying model's prediction of next token, by appending (learnable) pause tokens to allow increased inference-time computations .\n",
      "Keywords: LLM training and inference; Downstream finetuning\n",
      "---\n",
      "Title: Talk like a Graph: Encoding Graphs for Large Language Models\n",
      "Authors: Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Graph problems; large language models; encoding graphs; generative models\n",
      "---\n",
      "Title: OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text\n",
      "Authors: Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba\n",
      "TL;DR: We open-source a large-scale mathematical dataset extracted from the web.\n",
      "Keywords: web-scale dataset; natural language processing; large language model; reasoning; AI for math\n",
      "---\n",
      "Title: Vision-by-Language for Training-Free Compositional Image Retrieval\n",
      "Authors: Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata\n",
      "TL;DR: A simple method using off-the-shelf foundation models for Composed Image Retrieval without any training\n",
      "Keywords: Vision-Language Models; Large Language Models\n",
      "---\n",
      "Title: Chain-of-Experts: When LLMs Meet Complex Operations Research Problems\n",
      "Authors: Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, Gang Chen\n",
      "TL;DR: This paper introduces Chain-of-Experts (CoE), a multi-agent LLM framework that boosts reasoning in complex operation research problems by integrating domain-specific agents under a conductor's guidance and reflection mechanism.\n",
      "Keywords: Large Language Model; Operations Research\n",
      "---\n",
      "Title: GAIA: a benchmark for General AI Assistants\n",
      "Authors: Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, Thomas Scialom\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models; Benchmark\n",
      "---\n",
      "Title: SALMONN: Towards Generic Hearing Abilities for Large Language Models\n",
      "Authors: Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, Chao Zhang\n",
      "TL;DR: SALMONN: Towards Generic Hearing Abilities for Large Language Models\n",
      "Keywords: Multimodal large language models; speech and audio processing; music processing\n",
      "---\n",
      "Title: Magnushammer: A Transformer-Based Approach to Premise Selection\n",
      "Authors: Maciej Mikuła, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Q. Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, Yuhuai Wu\n",
      "TL;DR: Contrastively trained transformers outperform state-of-the-art symbolic methods for premise selection, a challenging reasoning task of selecting relevant facts for proving new theorems in formal mathematics.\n",
      "Keywords: transformers; interactive theorem proving; automated reasoning; contrastive learning; premise selection\n",
      "---\n",
      "Title: Learning to Compose: Improving Object Centric Learning by Injecting Compositionality\n",
      "Authors: Whie Jung, Jaehoon Yoo, Sungjin Ahn, Seunghoon Hong\n",
      "TL;DR: We propose a novel objective that explicitly encourages compositionality of the representations.\n",
      "Keywords: Object-Centric learning; Compositionality\n",
      "---\n",
      "Title: Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning\n",
      "Authors: LINHAO LUO, Yuan-Fang Li, Reza Haf, Shirui Pan\n",
      "TL;DR: we propose a novel method called Reasoning on Graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning.\n",
      "Keywords: large language models; knowledge graphs; reasoning\n",
      "---\n",
      "Title: Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World\n",
      "Authors: Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun Zhu, Yizhou Wang\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Few-shot learning; Visual reasoning; Open world learning\n",
      "---\n",
      "Title: The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models\n",
      "Authors: Raphaël Avalos, Florent Delgrange, Ann Nowe, Guillermo Perez, Diederik M Roijers\n",
      "TL;DR: Wasserstein Belief Updater is an RNN free RL algorithm for POMDPs that learns a representation of the history via an approximation of the belief update in a reliable latent space model, providing theoretical guarantees for learning the optimal value.\n",
      "Keywords: pomdp; guarantees; representation learning; reinforcement learning\n",
      "---\n",
      "Title: Turning large language models into cognitive models\n",
      "Authors: Marcel Binz, Eric Schulz\n",
      "TL;DR: We finetune a large language model on data from psychological experiments and find that doing so produces models that are more aligned with human decision-making.\n",
      "Keywords: cognitive modeling; large language models; neural networks; cognitive psychology; decision-making\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for paper_type, notes in papers_by_type.items():\n",
    "    print(f\"\\n{paper_type} Papers:\")\n",
    "    if notes:\n",
    "        for note in notes: \n",
    "            title = note.get('content', {}).get('title', 'No title available').get('value')\n",
    "            authors = \", \".join(note.get('content', {}).get('authors', []).get('value'))\n",
    "            abstract = note.get('content', {}).get('abstract', 'No abstract available').get('value')\n",
    "\n",
    "            tldr = note.get('content', {}).get('TLDR', '')\n",
    "            if tldr:\n",
    "                tldr = tldr['value']\n",
    "            else:\n",
    "                tldr = 'No TL;DR available'\n",
    "\n",
    "            keywords = note.get('content', {}).get('keywords', 'No keywords available').get('value')\n",
    "            keywords = '; '.join(keywords)\n",
    "\n",
    "\n",
    "            if ('reasoning' in tldr.lower()) or ('reasoning' in keywords.lower()) or ('reasoning' in abstract.lower()) or ('reasoning' in title.lower()):\n",
    "                # print(f\"Reasoning Paper\")\n",
    "\n",
    "                # link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "                print(f\"Title: {title}\")\n",
    "                print(f\"Authors: {authors}\")\n",
    "                # print(f\"Abstract: {abstract}\")\n",
    "                print(f\"TL;DR: {tldr}\")\n",
    "                print(f\"Keywords: {keywords}\")\n",
    "                # print(f\"Link: {link}\")\n",
    "                print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test the API endpoint directly and print a portion of the response\n",
    "url = \"https://api2.openreview.net/notes?content.venueid=ICLR.cc/2024/Conference\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Print the entire response to understand its structure\n",
    "print(json.dumps(data, indent=4))\n",
    "\n",
    "# Check if specific expected keys are present and print the first few entries if available\n",
    "if 'notes' in data and data['notes']:\n",
    "    for note in data['notes'][:5]:  # Print details of the first 5 entries\n",
    "        print(\"Title:\", note.get('content', {}).get('title', 'No title available'))\n",
    "        print(\"Authors:\", \", \".join(note.get('content', {}).get('authors', [])))\n",
    "        print(\"Abstract:\", note.get('content', {}).get('abstract', 'No abstract available'))\n",
    "        print(\"Link:\", f\"https://openreview.net/forum?id={note['id']}\")\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"No notes found in the data.\")\n",
    "\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Request a broad set of data to understand its structure\n",
    "# url = \"https://api2.openreview.net/notes?content.venueid=ICLR.cc/2024/Conference\"\n",
    "# response = requests.get(url)\n",
    "# data = response.json()\n",
    "\n",
    "# # Print a comprehensive view of the first few entries to inspect the structure\n",
    "# print(json.dumps(data['notes'][:5], indent=4))  # Print details of the first 5 entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"id\": \"vq11gurmUY\",\n",
      "        \"number\": 15599,\n",
      "        \"cdate\": 1683835182036,\n",
      "        \"tcdate\": 1683835182036,\n",
      "        \"mdate\": 1698949793552,\n",
      "        \"tmdate\": 1698949793552,\n",
      "        \"signatures\": [\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15599/Authors\"\n",
      "        ],\n",
      "        \"readers\": [\n",
      "            \"everyone\"\n",
      "        ],\n",
      "        \"writers\": [\n",
      "            \"NeurIPS.cc/2023/Conference\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15599/Authors\"\n",
      "        ],\n",
      "        \"forum\": \"vq11gurmUY\",\n",
      "        \"content\": {\n",
      "            \"title\": {\n",
      "                \"value\": \"Online PCA in Converging Self-consistent Field Equations\"\n",
      "            },\n",
      "            \"authors\": {\n",
      "                \"value\": [\n",
      "                    \"Xihan Li\",\n",
      "                    \"Xiang Chen\",\n",
      "                    \"Rasul Tutunov\",\n",
      "                    \"Haitham Bou Ammar\",\n",
      "                    \"Lei Wang\",\n",
      "                    \"Jun Wang\"\n",
      "                ]\n",
      "            },\n",
      "            \"authorids\": {\n",
      "                \"value\": [\n",
      "                    \"~Xihan_Li1\",\n",
      "                    \"~Xiang_Chen8\",\n",
      "                    \"~Rasul_Tutunov3\",\n",
      "                    \"~Haitham_Bou_Ammar1\",\n",
      "                    \"~Lei_Wang3\",\n",
      "                    \"~Jun_Wang2\"\n",
      "                ]\n",
      "            },\n",
      "            \"keywords\": {\n",
      "                \"value\": [\n",
      "                    \"Self-consistent Field Equation\",\n",
      "                    \"Computational Science\",\n",
      "                    \"Online PCA\"\n",
      "                ]\n",
      "            },\n",
      "            \"TLDR\": {\n",
      "                \"value\": \"We developed a new algorithm based on online PCA to converge Self-consistent Field Equations\"\n",
      "            },\n",
      "            \"abstract\": {\n",
      "                \"value\": \"Self-consistent Field (SCF) equation is a type of nonlinear eigenvalue problem in which the matrix to be eigen-decomposed is a function of its own eigenvectors. It is of great significance in computational science for its connection to the Schr\\u00f6dinger equation. Traditional fixed-point iteration methods for solving such equations suffer from non-convergence issues. In this work, we present a novel perspective on such SCF equations as a principal component analysis (PCA) for non-stationary time series, in which a distribution and its own top principal components are mutually updated over time, and the equilibrium state of the model corresponds to the solution of the SCF equations. By the new perspective, online PCA techniques are able to engage in so as to enhance the convergence of the model towards the equilibrium state, acting as a new set of tools for converging the SCF equations. With several numerical adaptations, we then develop a new algorithm for converging the SCF equation, and demonstrated its high convergence capacity with experiments on both synthesized and real electronic structure scenarios.\"\n",
      "            },\n",
      "            \"venue\": {\n",
      "                \"value\": \"NeurIPS 2023 poster\"\n",
      "            },\n",
      "            \"venueid\": {\n",
      "                \"value\": \"NeurIPS.cc/2023/Conference\"\n",
      "            },\n",
      "            \"pdf\": {\n",
      "                \"value\": \"/pdf/7e20e8d644a62ef06a3a146de449a37c5f5c8180.pdf\"\n",
      "            },\n",
      "            \"supplementary_material\": {\n",
      "                \"value\": \"/attachment/48a09f16be2046bba35d54437a304db1be58641e.pdf\"\n",
      "            },\n",
      "            \"_bibtex\": {\n",
      "                \"value\": \"@inproceedings{\\nli2023online,\\ntitle={Online {PCA} in Converging Self-consistent Field Equations},\\nauthor={Xihan Li and Xiang Chen and Rasul Tutunov and Haitham Bou Ammar and Lei Wang and Jun Wang},\\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\\nyear={2023},\\nurl={https://openreview.net/forum?id=vq11gurmUY}\\n}\"\n",
      "            },\n",
      "            \"paperhash\": {\n",
      "                \"value\": \"li|online_pca_in_converging_selfconsistent_field_equations\"\n",
      "            }\n",
      "        },\n",
      "        \"invitations\": [\n",
      "            \"NeurIPS.cc/2023/Conference/-/Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Post_Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15599/-/Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/PC_Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15599/-/Supplementary_Material_Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Edit\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15599/-/Camera_Ready_Revision\"\n",
      "        ],\n",
      "        \"domain\": \"NeurIPS.cc/2023/Conference\",\n",
      "        \"pdate\": 1695326179222,\n",
      "        \"odate\": 1698949793538,\n",
      "        \"version\": 2\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"zyZkaqNnpa\",\n",
      "        \"number\": 15594,\n",
      "        \"cdate\": 1683835167534,\n",
      "        \"tcdate\": 1683835167534,\n",
      "        \"mdate\": 1698949793550,\n",
      "        \"tmdate\": 1698949793550,\n",
      "        \"signatures\": [\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15594/Authors\"\n",
      "        ],\n",
      "        \"readers\": [\n",
      "            \"everyone\"\n",
      "        ],\n",
      "        \"writers\": [\n",
      "            \"NeurIPS.cc/2023/Conference\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15594/Authors\"\n",
      "        ],\n",
      "        \"forum\": \"zyZkaqNnpa\",\n",
      "        \"content\": {\n",
      "            \"title\": {\n",
      "                \"value\": \"Don\\u2019t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy\"\n",
      "            },\n",
      "            \"authors\": {\n",
      "                \"value\": [\n",
      "                    \"Aahlad Manas Puli\",\n",
      "                    \"Lily H Zhang\",\n",
      "                    \"Yoav Wald\",\n",
      "                    \"Rajesh Ranganath\"\n",
      "                ]\n",
      "            },\n",
      "            \"authorids\": {\n",
      "                \"value\": [\n",
      "                    \"~Aahlad_Manas_Puli1\",\n",
      "                    \"~Lily_H_Zhang1\",\n",
      "                    \"~Yoav_Wald1\",\n",
      "                    \"~Rajesh_Ranganath2\"\n",
      "                ]\n",
      "            },\n",
      "            \"keywords\": {\n",
      "                \"value\": [\n",
      "                    \"shortcut learning\",\n",
      "                    \"spurious correlations\",\n",
      "                    \"perfect stable feature\",\n",
      "                    \"perception tasks\",\n",
      "                    \"implicit bias in optimization\",\n",
      "                    \"improving inductive biases\"\n",
      "                ]\n",
      "            },\n",
      "            \"TLDR\": {\n",
      "                \"value\": \"Implicit biases toward maximizing margins induce shortcut learning in ERM even in tasks with perfect stable features, controlling margins mitigates shortcuts\"\n",
      "            },\n",
      "            \"abstract\": {\n",
      "                \"value\": \"Common explanations for shortcut learning assume that the shortcut improves prediction only under the training distribution. Thus, models trained in the typical way by minimizing log-loss using gradient descent, which we call default-ERM, should utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-ERM exhibits shortcut learning. Why are such solutions preferred when the loss can be driven to zero when using the stable feature alone? By studying a linear perception task, we show that default-ERM\\u2019s preference for maximizing the margin, even without overparameterization, leads to models that depend more on the shortcut than the stable feature. This insight suggests that default-ERM\\u2019s implicit inductive bias towards max-margin may be unsuitable for perception tasks. Instead, we consider inductive biases toward uniform margins. We show that uniform margins guarantee sole dependence on the perfect stable feature in the linear perception task and suggest alternative loss functions, termed margin control (MARG-CTRL), that encourage uniform-margin solutions. MARG-CTRL techniques mitigate shortcut learning on a variety of vision and language tasks, showing that changing inductive biases can remove the need for complicated shortcut-mitigating methods in perception tasks.\"\n",
      "            },\n",
      "            \"venue\": {\n",
      "                \"value\": \"NeurIPS 2023 poster\"\n",
      "            },\n",
      "            \"venueid\": {\n",
      "                \"value\": \"NeurIPS.cc/2023/Conference\"\n",
      "            },\n",
      "            \"pdf\": {\n",
      "                \"value\": \"/pdf/956a72600022df2e6738151ad2ff6a638cc32154.pdf\"\n",
      "            },\n",
      "            \"_bibtex\": {\n",
      "                \"value\": \"@inproceedings{\\npuli2023dont,\\ntitle={Don{\\\\textquoteright}t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy},\\nauthor={Aahlad Manas Puli and Lily H Zhang and Yoav Wald and Rajesh Ranganath},\\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\\nyear={2023},\\nurl={https://openreview.net/forum?id=zyZkaqNnpa}\\n}\"\n",
      "            },\n",
      "            \"paperhash\": {\n",
      "                \"value\": \"puli|dont_blame_dataset_shift_shortcut_learning_due_to_gradients_and_cross_entropy\"\n",
      "            }\n",
      "        },\n",
      "        \"invitations\": [\n",
      "            \"NeurIPS.cc/2023/Conference/-/Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Post_Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15594/-/Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15594/-/Supplementary_Material_Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Edit\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15594/-/Camera_Ready_Revision\"\n",
      "        ],\n",
      "        \"domain\": \"NeurIPS.cc/2023/Conference\",\n",
      "        \"pdate\": 1695326179171,\n",
      "        \"odate\": 1698949793534,\n",
      "        \"version\": 2\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"JMuKfZx2xU\",\n",
      "        \"number\": 15586,\n",
      "        \"cdate\": 1683835147191,\n",
      "        \"tcdate\": 1683835147191,\n",
      "        \"mdate\": 1698949793495,\n",
      "        \"tmdate\": 1698949793495,\n",
      "        \"signatures\": [\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15586/Authors\"\n",
      "        ],\n",
      "        \"readers\": [\n",
      "            \"everyone\"\n",
      "        ],\n",
      "        \"writers\": [\n",
      "            \"NeurIPS.cc/2023/Conference\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15586/Authors\"\n",
      "        ],\n",
      "        \"forum\": \"JMuKfZx2xU\",\n",
      "        \"content\": {\n",
      "            \"title\": {\n",
      "                \"value\": \"On Slicing Optimality for Mutual Information\"\n",
      "            },\n",
      "            \"authors\": {\n",
      "                \"value\": [\n",
      "                    \"Ammar Fayad\",\n",
      "                    \"Majd Ibrahim\"\n",
      "                ]\n",
      "            },\n",
      "            \"authorids\": {\n",
      "                \"value\": [\n",
      "                    \"~Ammar_Fayad1\",\n",
      "                    \"~Majd_Ibrahim1\"\n",
      "                ]\n",
      "            },\n",
      "            \"keywords\": {\n",
      "                \"value\": [\n",
      "                    \"Mutual information\",\n",
      "                    \"Information Theory\"\n",
      "                ]\n",
      "            },\n",
      "            \"abstract\": {\n",
      "                \"value\": \"Measuring dependence between two random variables is of great importance in various domains but is difficult to compute in today's complex environments with high-dimensional data. Recently, slicing methods have shown to be a scalable approach to measuring mutual information (MI) between high-dimensional variables by projecting these variables into one-dimensional spaces. Unfortunately, these methods use uniform distributions of slicing directions, which generally discard informative features between variables and thus lead to inaccurate quantification of dependence. In this paper, we propose a principled framework that searches for an \\\\textit{optimal} distribution of slices for MI. Importantly, we answer theoretical questions about finding the optimal slicing distribution in the context of MI and develop corresponding theoretical analyses. We also develop a practical algorithm, connecting our theoretical results with modern machine learning frameworks. Through comprehensive experiments in benchmark domains, we demonstrate significant gains in our information measure than state-of-the-art baselines.\"\n",
      "            },\n",
      "            \"venue\": {\n",
      "                \"value\": \"NeurIPS 2023 poster\"\n",
      "            },\n",
      "            \"venueid\": {\n",
      "                \"value\": \"NeurIPS.cc/2023/Conference\"\n",
      "            },\n",
      "            \"pdf\": {\n",
      "                \"value\": \"/pdf/6a227cc45293972b8bb2ac48c87b3543ba961bca.pdf\"\n",
      "            },\n",
      "            \"supplementary_material\": {\n",
      "                \"value\": \"/attachment/3d7027975c56990a4684efd55b0d2053aface9c1.pdf\"\n",
      "            },\n",
      "            \"_bibtex\": {\n",
      "                \"value\": \"@inproceedings{\\nfayad2023on,\\ntitle={On Slicing Optimality for Mutual Information},\\nauthor={Ammar Fayad and Majd Ibrahim},\\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\\nyear={2023},\\nurl={https://openreview.net/forum?id=JMuKfZx2xU}\\n}\"\n",
      "            },\n",
      "            \"paperhash\": {\n",
      "                \"value\": \"fayad|on_slicing_optimality_for_mutual_information\"\n",
      "            }\n",
      "        },\n",
      "        \"invitations\": [\n",
      "            \"NeurIPS.cc/2023/Conference/-/Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Post_Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15586/-/Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15586/-/Supplementary_Material_Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Edit\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15586/-/Camera_Ready_Revision\"\n",
      "        ],\n",
      "        \"domain\": \"NeurIPS.cc/2023/Conference\",\n",
      "        \"pdate\": 1695326179026,\n",
      "        \"odate\": 1698949793480,\n",
      "        \"version\": 2\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"9zV2OXCrVF\",\n",
      "        \"number\": 15580,\n",
      "        \"cdate\": 1683835132850,\n",
      "        \"tcdate\": 1683835132850,\n",
      "        \"mdate\": 1698949793456,\n",
      "        \"tmdate\": 1698949793456,\n",
      "        \"signatures\": [\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15580/Authors\"\n",
      "        ],\n",
      "        \"readers\": [\n",
      "            \"everyone\"\n",
      "        ],\n",
      "        \"writers\": [\n",
      "            \"NeurIPS.cc/2023/Conference\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15580/Authors\"\n",
      "        ],\n",
      "        \"forum\": \"9zV2OXCrVF\",\n",
      "        \"content\": {\n",
      "            \"title\": {\n",
      "                \"value\": \"k-Median Clustering via Metric Embedding: Towards Better Initialization with Differential Privacy\"\n",
      "            },\n",
      "            \"authors\": {\n",
      "                \"value\": [\n",
      "                    \"Chenglin Fan\",\n",
      "                    \"Ping Li\",\n",
      "                    \"Xiaoyun Li\"\n",
      "                ]\n",
      "            },\n",
      "            \"authorids\": {\n",
      "                \"value\": [\n",
      "                    \"~Chenglin_Fan2\",\n",
      "                    \"~Ping_Li3\",\n",
      "                    \"~Xiaoyun_Li2\"\n",
      "                ]\n",
      "            },\n",
      "            \"keywords\": {\n",
      "                \"value\": [\n",
      "                    \"privacy\",\n",
      "                    \"clustering\"\n",
      "                ]\n",
      "            },\n",
      "            \"abstract\": {\n",
      "                \"value\": \"In clustering algorithms, the choice of initial centers is crucial for the quality of the learned clusters. We propose a new initialization scheme for the $k$-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. We propose a novel and efficient search algorithm, for good initial centers that can be used subsequently for the local search algorithm. The so-called HST initialization method can produce initial centers achieving lower error than those from another popular method $k$-median++, also with higher efficiency when $k$ is not too small. Our HST initialization can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. We show that the error of applying DP local search followed by our private HST initialization improves previous results on the approximation error, and approaches the lower bound within a small factor. Experiments demonstrate the effectiveness of our proposed methods.\"\n",
      "            },\n",
      "            \"venue\": {\n",
      "                \"value\": \"NeurIPS 2023 poster\"\n",
      "            },\n",
      "            \"venueid\": {\n",
      "                \"value\": \"NeurIPS.cc/2023/Conference\"\n",
      "            },\n",
      "            \"pdf\": {\n",
      "                \"value\": \"/pdf/8dca3ac9fe5a6e2ffca79095482b93395aae29f9.pdf\"\n",
      "            },\n",
      "            \"_bibtex\": {\n",
      "                \"value\": \"@inproceedings{\\nfan2023kmedian,\\ntitle={k-Median Clustering via Metric Embedding: Towards Better Initialization with Differential Privacy},\\nauthor={Chenglin Fan and Ping Li and Xiaoyun Li},\\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\\nyear={2023},\\nurl={https://openreview.net/forum?id=9zV2OXCrVF}\\n}\"\n",
      "            },\n",
      "            \"paperhash\": {\n",
      "                \"value\": \"fan|kmedian_clustering_via_metric_embedding_towards_better_initialization_with_differential_privacy\"\n",
      "            }\n",
      "        },\n",
      "        \"invitations\": [\n",
      "            \"NeurIPS.cc/2023/Conference/-/Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Post_Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15580/-/Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15580/-/Supplementary_Material_Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Edit\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15580/-/Camera_Ready_Revision\"\n",
      "        ],\n",
      "        \"domain\": \"NeurIPS.cc/2023/Conference\",\n",
      "        \"pdate\": 1695326178879,\n",
      "        \"odate\": 1698949793442,\n",
      "        \"version\": 2\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"CAF4CnUblx\",\n",
      "        \"forum\": \"CAF4CnUblx\",\n",
      "        \"number\": 15576,\n",
      "        \"cdate\": 1683835114001,\n",
      "        \"tcdate\": 1683835114001,\n",
      "        \"mdate\": 1705359578013,\n",
      "        \"tmdate\": 1705359578013,\n",
      "        \"signatures\": [\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15576/Authors\"\n",
      "        ],\n",
      "        \"readers\": [\n",
      "            \"everyone\"\n",
      "        ],\n",
      "        \"writers\": [\n",
      "            \"NeurIPS.cc/2023/Conference\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15576/Authors\"\n",
      "        ],\n",
      "        \"content\": {\n",
      "            \"title\": {\n",
      "                \"value\": \"Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI\"\n",
      "            },\n",
      "            \"authors\": {\n",
      "                \"value\": [\n",
      "                    \"Aditya Chattopadhyay\",\n",
      "                    \"Ryan Pilgrim\",\n",
      "                    \"Rene Vidal\"\n",
      "                ]\n",
      "            },\n",
      "            \"authorids\": {\n",
      "                \"value\": [\n",
      "                    \"~Aditya_Chattopadhyay1\",\n",
      "                    \"~Ryan_Pilgrim1\",\n",
      "                    \"~Rene_Vidal1\"\n",
      "                ]\n",
      "            },\n",
      "            \"keywords\": {\n",
      "                \"value\": [\n",
      "                    \"Information Maximization\",\n",
      "                    \"Sparse Coding\",\n",
      "                    \"Orthogonal Matching Pursuit\",\n",
      "                    \"Explainable AI\",\n",
      "                    \"Information Pursuit\"\n",
      "                ]\n",
      "            },\n",
      "            \"abstract\": {\n",
      "                \"value\": \"Information Pursuit (IP) is a classical active testing algorithm for predicting an output by sequentially and greedily querying the input in order of information gain. However, IP is computationally intensive since it involves estimating mutual information in high-dimensional spaces. This paper explores Orthogonal Matching Pursuit (OMP) as an alternative to IP for greedily selecting the queries. OMP is a classical signal processing algorithm for sequentially encoding a signal in terms of dictionary atoms chosen in order of correlation gain. In each iteration, OMP selects the atom that is most correlated with the signal residual (the signal minus its reconstruction thus far). Our first contribution is to establish a fundamental connection between IP and OMP, where we prove that IP with random projections of dictionary atoms as queries ``almost'' reduces to OMP, with the difference being that IP selects atoms in order of normalized correlation gain. We call this version IP-OMP and present simulations indicating that this difference does not have any appreciable effect on the sparse code recovery rate of IP-OMP compared to that of OMP for random Gaussian dictionaries. Inspired by this connection, our second contribution is to explore the utility of IP-OMP for generating explainable predictions, an area in which IP has recently gained traction. More specifically, we propose a simple explainable AI algorithm which encodes an image as a sparse combination of semantically meaningful dictionary atoms that are defined as text embeddings of interpretable concepts. The final prediction is made using the weights of this sparse combination, which serve as an explanation. Empirically, our proposed algorithm is not only competitive with existing explainability methods but also computationally less expensive.\"\n",
      "            },\n",
      "            \"venue\": {\n",
      "                \"value\": \"NeurIPS 2023 spotlight\"\n",
      "            },\n",
      "            \"venueid\": {\n",
      "                \"value\": \"NeurIPS.cc/2023/Conference\"\n",
      "            },\n",
      "            \"pdf\": {\n",
      "                \"value\": \"/pdf/ff73136503055cc3efd6b862247076fe67720eb3.pdf\"\n",
      "            },\n",
      "            \"TLDR\": {\n",
      "                \"value\": \"We show that the popular OMP algorithm can be derived from information-theoretic principles modulo a normalization factor. We then use this insight to design a computationally simple sparse-coding based explainable AI algorithm.\"\n",
      "            },\n",
      "            \"_bibtex\": {\n",
      "                \"value\": \"@inproceedings{\\nchattopadhyay2023information,\\ntitle={Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable {AI}},\\nauthor={Aditya Chattopadhyay and Ryan Pilgrim and Rene Vidal},\\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\\nyear={2023},\\nurl={https://openreview.net/forum?id=CAF4CnUblx}\\n}\"\n",
      "            },\n",
      "            \"paperhash\": {\n",
      "                \"value\": \"chattopadhyay|information_maximization_perspective_of_orthogonal_matching_pursuit_with_applications_to_explainable_ai\"\n",
      "            }\n",
      "        },\n",
      "        \"pdate\": 1695326178872,\n",
      "        \"invitations\": [\n",
      "            \"NeurIPS.cc/2023/Conference/-/Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Post_Submission\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15576/-/Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15576/-/Supplementary_Material_Revision\",\n",
      "            \"NeurIPS.cc/2023/Conference/-/Edit\",\n",
      "            \"NeurIPS.cc/2023/Conference/Submission15576/-/Camera_Ready_Revision\"\n",
      "        ],\n",
      "        \"domain\": \"NeurIPS.cc/2023/Conference\",\n",
      "        \"odate\": 1698949793398,\n",
      "        \"version\": 2\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Request a broad set of data to understand its structure\n",
    "url = \"https://api2.openreview.net/notes?content.venueid=NeurIPS.cc/2023/Conference\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Print a comprehensive view of the first few entries to inspect the structure\n",
    "print(json.dumps(data['notes'][:5], indent=4))  # Print details of the first 5 entries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
